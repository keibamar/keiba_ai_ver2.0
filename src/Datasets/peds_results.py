import os
import re
import sys

from datetime import date, timedelta
from glob import glob
import numpy as np
import pandas as pd
from tqdm import tqdm

# pycache ã‚’ç”Ÿæˆã—ãªã„
sys.dont_write_bytecode = True
sys.path.append(r"C:\keiba_ai\keiba_ai_ver2.0\libs")
import get_race_id
import name_header
import horse_peds
import race_results
import scraping
import past_performance

def peds_dataset_error(e):
    """ ã‚¨ãƒ©ãƒ¼æ™‚å‹•ä½œã‚’è¨˜è¼‰ã™ã‚‹ 
        Args:
            e (Exception) : ã‚¨ãƒ©ãƒ¼å†…å®¹ 
    """
    print(__name__ + ":" + __file__)
    print(f"{e.__class__.__name__}: {e}")

def make_peds_dataset_from_race_results(place_id, year):
    """ peds_datasetã‚’race_resultsã‹ã‚‰ä½œæˆã™ã‚‹ 
        Args:
            place_id (int) : é–‹å‚¬ã‚³ãƒ¼ã‚¹id
            year(int) : é–‹å‚¬å¹´
    """
    #ãƒ¬ãƒ¼ã‚¹çµæœã®csvæƒ…å ±ã‚’å–å¾—
    df_course = race_results.get_race_results_csv(place_id, year)
    
    if not df_course.empty:
        # ãƒ›ãƒ¼ã‚¹IDã®æŠ½å‡º
        horse_id_list = df_course["horse_id"].to_list()

        # è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        df_peds = get_peds_dataset_from_horse_id_list(horse_id_list)
        
        # datasetã®ä¿å­˜
        save_peds_dataset(df_peds, place_id, year)

def save_peds_dataset(peds_df, place_id, year):
    """ peds_datasetã®DataFrameã‚’ä¿å­˜ 
        Args:
            peds_df(pd.DataFrame)
            horse_id (int) :horse_id
            past_performance_df.DataFrameï¼‰ : past_performanceã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    """
    try:
        if any(peds_df):
            # é‡è¤‡ã—ã¦ã„ã‚‹å†…å®¹ã‚’æ¶ˆå»
            peds_df = peds_df.drop_duplicates(keep = 'first')
            # strå‹ã«å¤‰æ›´
            peds_df = peds_df.astype(str)
            # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
            peds_df.to_csv(name_header.DATA_PATH + "/PedsResults/" +  name_header.PLACE_LIST[place_id - 1] + '//' + str(year) + '_peds.csv')
            peds_df.to_pickle(name_header.DATA_PATH + "/PedsResults/" + name_header.PLACE_LIST[place_id - 1] + '//' + str(year) + '_peds.pickle')
    except Exception as e:
            peds_dataset_error(e)

def get_peds_dataset_csv(place_id, year):
    """ pedsã®csvã‚’å–å¾—ã™ã‚‹ 
        Args:
            place_id (int) : é–‹å‚¬ã‚³ãƒ¼ã‚¹id
            year(int) : é–‹å‚¬å¹´
    """
    # csvã‚’èª­ã¿è¾¼ã‚€ 
    path = name_header.DATA_PATH + "/PedsResults/" +  name_header.PLACE_LIST[place_id - 1] + '//' + str(year) + '_peds.csv'
    if os.path.isfile(path):
        df = pd.read_csv(path, index_col = 0, dtype = str)
        df.fillna(np.nan)  
    else :
        df = pd.DataFrame()
    return df

def get_peds_data_dataset_csv(place_id, year):
    """ peds_dataã®csvã‚’å–å¾—ã™ã‚‹ 
        Args:
            place_id (int) : é–‹å‚¬ã‚³ãƒ¼ã‚¹id
            year(int) : é–‹å‚¬å¹´
    """
    # csvã‚’èª­ã¿è¾¼ã‚€ 
    path = name_header.DATA_PATH + "/PedsResults/" +  name_header.PLACE_LIST[place_id - 1] + '//' + str(year) + '_peds_data.csv'
    if os.path.isfile(path):
        df = pd.read_csv(path, index_col = 0, dtype = str)
        df.fillna(np.nan)  
    else :
        df = pd.DataFrame()
    return df

def get_peds_dataset_from_horse_id_list(horse_id_list):
    """ horse_id_listã‹ã‚‰peds_datasetã®Datasetã‚’ä½œæˆ 
        Args:
            horse_id_list : horse_idã®ãƒªã‚¹ãƒˆ
        Returns:
            peds_df(pd.DataFrame) : peds_dataset
    """
    # è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    peds_df = pd.DataFrame()
    for horse_id in horse_id_list:
        peds_df = pd.concat([peds_df,horse_peds.get_horse_peds_csv(horse_id).T],axis = 0)
    return peds_df

def merge_pedsdata_with_race_results(place_id, year):
    """ è¡€çµ±ã¨ãƒ¬ãƒ¼ã‚¹çµæœã®çµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆ 
        Args:
            place_id (int) : é–‹å‚¬ã‚³ãƒ¼ã‚¹id
            year(int) : é–‹å‚¬å¹´
    """
    #ãƒ¬ãƒ¼ã‚¹çµæœã®csvæƒ…å ±ã‚’å–å¾—
    df_course = race_results.get_race_results_csv(place_id, year)
    if not df_course.empty:
        # ãƒ¬ãƒ¼ã‚¹çµæœã‹ã‚‰ï¼Œ"ç€é †","race_type","course_len","ground_state","class"ã‚’æŠ½å‡º
        df_result = df_course['ç€é †']
        df_result =pd.concat([df_result,df_course['race_type']],axis = 1)
        df_result =pd.concat([df_result,df_course['course_len']],axis = 1)
        df_result =pd.concat([df_result,df_course['ground_state']],axis = 1)
        df_result =pd.concat([df_result,df_course['class']],axis = 1)

        # è¡€çµ±ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        df_peds = get_peds_dataset_csv(place_id, year)    
        # horse_idã«ä¸€è‡´ã™ã‚‹è¡€çµ±æƒ…å ±ã‚’æŠ½å‡º
        df_peds_result =pd.DataFrame()
        for id in range(len(df_result.index)):
            num = df_course.iloc[id,:]
            num = int(num['horse_id'])
            if num in df_peds.index:
                df_peds_result = pd.concat([df_peds_result,df_peds.loc[num,:]], axis=1)
            else :
                horse_peds_data = horse_peds.get_horse_peds_csv(str(num))
                if horse_peds_data.empty:
                    horse_peds_data = pd.DataFrame( np.nan, index=df_peds_result.index, columns = [num])
                df_peds_result = pd.concat([df_peds_result,horse_peds_data], axis=1)
       
        df_peds_result = df_peds_result.reset_index(drop = True).T

        # index/columnsã®æ•´ç†
        df_peds_result = df_peds_result.reset_index(drop = True)
        df_peds_result.columns = [f'peds_{i}' for i in range(len(df_peds_result.columns))]
        # è¡€çµ±æƒ…å ±ã‚’çµåˆ
        df_result = pd.concat([df_result.reset_index(drop = True), df_peds_result], axis = 1)
        df_result.index = df_course.index
        df_result.index.name = "index"
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¿å­˜
        df_result.to_csv(name_header.DATA_PATH + "/PedsResults/" +  name_header.PLACE_LIST[place_id - 1] + '//' + str(year) + '_peds_data.csv')
        df_result.to_pickle(name_header.DATA_PATH + "/PedsResults/" +  name_header.PLACE_LIST[place_id - 1] + '//' + str(year) + '_peds_data.pickle')

def calc_peds_placed_rate(peds_data):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ç€åº¦æ•°ã‚’è¨ˆç®—
        Args: 
            peds_data(pd.DatatFrame): è¡€çµ±æ¯ã®ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        Returns:
            placed_rate(pd.DataFrame):ç€åº¦æ•°
    """
    placed_rate = [peds_data[peds_data['ç€é †'] == '1']['ç€é †'].count(), 
                        peds_data[peds_data['ç€é †'] == '2']['ç€é †'].count(),
                        peds_data[peds_data['ç€é †'] == '3']['ç€é †'].count(),
                        len(peds_data) - (peds_data[peds_data['ç€é †'] == '1']['ç€é †'].count() + peds_data[peds_data['ç€é †'] == '2']['ç€é †'].count() + peds_data[peds_data['ç€é †'] == '3']['ç€é †'].count())]
    return placed_rate

def calc_peds_data(df_result, course_len, race_class):
    """è¡€çµ±ã®ç€åº¦æ•°ã‚’è¨ˆç®—
        Args: 
            df_result(pd.DatatFrame): è¡€çµ±æ¯ã®ãƒ¬ãƒ¼ã‚¹çµæœãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            course_len(str) : ã‚³ãƒ¼ã‚¹ã‚­ãƒ§ãƒª
            race_class(sre) : ã‚¯ãƒ©ã‚¹
        Returns:
            return_df(pd.DataFrame) : è¡€çµ±ç€åº¦æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    """
    return_data = []
    # åŒç«¶é¦¬å ´ã‚¹ã‚³ã‚¢
    return_data.append(calc_peds_placed_rate(df_result))

    # åŒã‚³ãƒ¼ã‚¹
    df_result = df_result[df_result['course_len'] == str(course_len)]
    return_data.append(calc_peds_placed_rate(df_result))

    # åŒæ¡ä»¶
    df_result = df_result[df_result['class'] == str(race_class)]
    return_data.append(calc_peds_placed_rate(df_result))

    # ãƒªã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–
    return_df = pd.DataFrame(zip(*return_data), columns = ["place", "course", "class"])

    return return_df

def get_race_type_data(df_result, race_type, ground_state):
    """df_resultã®ãƒ¬ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã¨é¦¬å ´çŠ¶æ…‹ã‚’æŠ½å‡º
        Args:
            df_result(pd.DataFrame) : race_resultãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            race_type(str) : ã‚³ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—
            ground_state(str) : é¦¬å ´çŠ¶æ…‹
        Returns:
            df_result(pd.DataFrame) : race_result
    """
    df_result = df_result[df_result['race_type'] == race_type]
    df_result = df_result[df_result['ground_state'] == ground_state]

    return df_result

def peds_index(father, mother_father, course_info, year):
    """è¡€çµ±æƒ…å ±ã®æŠ½å‡º
        Args: 
            father(str) : çˆ¶
            mother_father(str) : æ¯çˆ¶
            course_info : place_id, race_type, course_len, ground_state, race_class
            year(int) : å¹´
        Returns:
            return_df(pd.DataFrame) : è¡€çµ±ã®ç€åº¦æ•°ãƒ‡ãƒ¼ã‚¿
    """
    place_id = course_info[0]
    race_type = course_info[1]
    course_len = course_info[2]
    ground_state = course_info[3]
    race_class = course_info[4]

    df_peds = pd.DataFrame()
    # å„å¹´åº¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    for y in range(2019, int(year)):
        df_peds = pd.concat([df_peds, get_peds_data_dataset_csv(place_id, y)])
    df_peds = df_peds.reset_index(drop = True)
    
    # ç€åº¦æ•°ã‚’æ ¼ç´
    return_df = pd.DataFrame()
    
    # çˆ¶æƒ…å ±ã‚’ç¿’å¾—
    peds_data_father = df_peds[df_peds['peds_0'] == father]
    peds_data_father =  get_race_type_data(peds_data_father, race_type, ground_state)
    return_df = pd.concat([return_df,calc_peds_data(peds_data_father, course_len, race_class)],axis = 0)

    # æ¯æƒ…å ±ã‚’ç¿’å¾—
    peds_data_mother_father = df_peds[df_peds['peds_4'] == mother_father]
    peds_data_mother_father =  get_race_type_data(peds_data_mother_father, race_type, ground_state)
    return_df = pd.concat([return_df,calc_peds_data(peds_data_mother_father, course_len, race_class)],axis = 0)
    
    # çˆ¶Ã—æ¯çˆ¶æƒ…å ±ã‚’ç¿’å¾—
    peds_data = df_peds[df_peds['peds_0'] == father]
    peds_data = peds_data[peds_data['peds_4'] == mother_father]
    peds_data = get_race_type_data(peds_data, race_type, ground_state)
    return_df = pd.concat([return_df,calc_peds_data(peds_data, course_len, race_class)],axis = 0)

    return return_df

def output_results(df_peds_results):
    """è¡€çµ±ã”ã¨ã®ç€é †é›†è¨ˆã—ã¦CSVå‡ºåŠ›"""
    if df_peds_results.empty:
        return

    result_list = []
    for peds, sub_df in df_peds_results.groupby("peds_0"):
        first = (sub_df["ç€é †"] == 1).sum()
        second = (sub_df["ç€é †"] == 2).sum()
        third = (sub_df["ç€é †"] == 3).sum()
        others = ((sub_df["ç€é †"] > 3) & (sub_df["ç€é †"].notna())).sum()

        result_list.append({
            "è¡€çµ±": peds,
            "1ç€": first,
            "2ç€": second,
            "3ç€": third,
            "ç€å¤–": others
        })

    result_df = pd.DataFrame(result_list)
    result_df = result_df.sort_values(by=["1ç€", "2ç€", "3ç€"], ascending=False)

    return result_df

def aggregate_peds_results(place_id, year):
    """
    å„ã‚³ãƒ¼ã‚¹ï¼ˆèŠ/ãƒ€ãƒ¼ãƒˆãƒ»è·é›¢ï¼‰Ã— é¦¬å ´çŠ¶æ…‹ Ã— ã‚¯ãƒ©ã‚¹ã”ã¨ã«è¡€çµ±æˆç¸¾ã‚’é›†è¨ˆã€‚
    å…¨é¦¬å ´çŠ¶æ…‹(all)ã€å…¨ã‚¯ãƒ©ã‚¹(all)ã®é›†è¨ˆã‚‚åŒæ™‚ã«å‡ºåŠ›ã€‚
    """
    # === ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===
    df = get_peds_data_dataset_csv(place_id, year)
    if df.empty:
        print("not PedsResultData:", name_header.PLACE_LIST[place_id - 1], year)
        return
    
    output_dir = os.path.join(name_header.DATA_PATH, "PedsResults", name_header.PLACE_LIST[place_id - 1], str(year))
    os.makedirs(output_dir, exist_ok=True)

    # ç€é †ã‚’æ•°å€¤åŒ–
    df["ç€é †"] = pd.to_numeric(df["ç€é †"], errors="coerce")

    # === race_type, course_len ã”ã¨ã«å‡¦ç† ===
    for (race_type, course_len), df_group in df.groupby(["race_type", "course_len"]):
        ground_list = sorted(df_group["ground_state"].dropna().unique())

        # === å„é¦¬å ´çŠ¶æ…‹ã”ã¨ ===
        for ground_state in ground_list + ["all"]:
            if ground_state == "all":
                df_ground = df_group
            else:
                df_ground = df_group[df_group["ground_state"] == ground_state]

            if df_ground.empty:
                continue
            
            # === å…¨ã‚¯ãƒ©ã‚¹ã¾ã¨ã‚(all) ===
            result_all_classes = []
            result_all = output_results(df_ground)
            if not result_all.empty:
                result_all.insert(0, "ã‚¯ãƒ©ã‚¹", "all")
                result_all_classes.append(result_all)

            # === ã‚¯ãƒ©ã‚¹åˆ¥å‡¦ç† ===
            for class_name, df_class in df_ground.groupby("class"):
                result_df = output_results(df_class)
                if not result_df.empty:
                    result_df.insert(0, "ã‚¯ãƒ©ã‚¹", class_name)
                    result_all_classes.append(result_df)

            # === å…¨ã‚¯ãƒ©ã‚¹çµåˆ ===
            if result_all_classes:
                final_df = pd.concat(result_all_classes, ignore_index=True)
            else:
                continue

            # === å‡ºåŠ› ===
            file_name = f"{race_type}_{course_len}m_{ground_state}.csv"
            output_path = os.path.join(output_dir, file_name)
            final_df.to_csv(output_path, index=False, encoding="utf-8-sig")

        print(f"make peds_result {name_header.PLACE_LIST[place_id -1]} {year} {race_type} {course_len}m")


def aggregate_total_peds_results(place_id, start_year=2019, end_year=date.today().year):
    """
    å„å¹´åº¦ã®PedsçµæœCSVã‚’çµ±åˆã—ã€Totalãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«åˆè¨ˆçµæœã‚’å‡ºåŠ›ã€‚

    Parameters
    ----------
    place_id : int
        ç«¶é¦¬å ´ID
    start_year : int
        é›†è¨ˆé–‹å§‹å¹´
    end_year : int
        é›†è¨ˆçµ‚äº†å¹´ï¼ˆå«ã‚€ï¼‰
    """

    base_dir = os.path.join(name_header.DATA_PATH, "PedsResults", name_header.PLACE_LIST[place_id - 1])
    output_dir = os.path.join(base_dir, "Total")
    os.makedirs(output_dir, exist_ok=True)

    # === å…¨ãƒ•ã‚¡ã‚¤ãƒ«åã®é›†åˆã‚’å–å¾— ===
    all_files = set()
    for year in range(start_year, end_year + 1):
        year_dir = os.path.join(base_dir, str(year))
        if not os.path.exists(year_dir):
            continue
        csv_files = [os.path.basename(p) for p in glob(os.path.join(year_dir, "*.csv"))]
        all_files.update(csv_files)

    print(f"ğŸ” é›†è¨ˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_files)} ({start_year}â€“{end_year})")

    # === å„ãƒ•ã‚¡ã‚¤ãƒ«åã”ã¨ã«é›†è¨ˆ ===
    for file_name in sorted(all_files):
        merged_df_list = []

        for year in range(start_year, end_year + 1):
            csv_path = os.path.join(base_dir, str(year), file_name)
            if os.path.exists(csv_path):
                df = pd.read_csv(csv_path)
                if not df.empty:
                    df["year"] = year
                    merged_df_list.append(df)

        if not merged_df_list:
            continue

        df_all = pd.concat(merged_df_list, ignore_index=True)

        # === é›†è¨ˆ ===
        agg_df = (
            df_all.groupby(["ã‚¯ãƒ©ã‚¹", "è¡€çµ±"], as_index=False)[["1ç€", "2ç€", "3ç€", "ç€å¤–"]]
            .sum()
        )
        # === ã‚¯ãƒ©ã‚¹ã®è¡¨ç¤ºé †ã‚’å®šç¾© ===
        CLASS_ORDER = ["all", "æœªå‹åˆ©", "æ–°é¦¬", "1å‹ã‚¯ãƒ©ã‚¹", "2å‹ã‚¯ãƒ©ã‚¹", "3å‹ã‚¯ãƒ©ã‚¹", "ã‚ªãƒ¼ãƒ—ãƒ³"]

        # === ã‚¯ãƒ©ã‚¹é †åºã‚’ã‚«ãƒ†ã‚´ãƒªå‹ã§ä¿æŒ ===
        agg_df["ã‚¯ãƒ©ã‚¹"] = pd.Categorical(agg_df["ã‚¯ãƒ©ã‚¹"], categories=CLASS_ORDER, ordered=True)
        
        # === ã‚¯ãƒ©ã‚¹ â†’ ç€é † ã®é †ã§ã‚½ãƒ¼ãƒˆ ===
        agg_df = agg_df.sort_values(
            by=["ã‚¯ãƒ©ã‚¹", "1ç€", "2ç€", "3ç€"],
            ascending=[True, False, False, False]
        ).reset_index(drop=True)

        # === å‡ºåŠ› ===
        output_path = os.path.join(output_dir, file_name)
        agg_df.to_csv(output_path, index=False, encoding="utf-8-sig")

        print(f"âœ… Totalé›†è¨ˆå®Œäº†: {file_name}")

    print(f"\nğŸ¯ ã™ã¹ã¦ã®Totalé›†è¨ˆãŒå®Œäº†ã—ã¾ã—ãŸ -> {output_dir}")
        

def update_peds_dataset(place_id, day = date.today()):
    """ æŒ‡å®šã—ãŸã‚³ãƒ¼ã‚¹ã®æŒ‡å®šæ—¥ã‹ã‚‰ã€ï¼‘é€±é–“åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ›´æ–°  
    Args:
        place_id (int) : é–‹å‚¬ã‚³ãƒ¼ã‚¹id
        day(Date) : æ—¥ï¼ˆåˆæœŸå€¤ï¼šä»Šæ—¥ï¼‰
    """ 
    race_id_list = get_race_id.get_past_weekly_id(place_id, day)
    horse_id_list = past_performance.get_horse_id_list_from_race_id_list(race_id_list)
    if any(horse_id_list):
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å–å¾—
        new_peds_df = get_peds_dataset_from_horse_id_list(horse_id_list)
        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
        old_peds_df = get_peds_dataset_csv(place_id, day.year)
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ±åˆ
        print(old_peds_df)
        new_peds_df = pd.concat([old_peds_df,new_peds_df],axis = 0)
        print(new_peds_df)
        save_peds_dataset(new_peds_df, place_id, day.year)
        # é›†è¨ˆçµæœã‚’æ›´æ–°
        aggregate_peds_results(place_id, day.year)
        aggregate_total_peds_results(place_id = place_id, end_year = day.year)
    
def weekly_update_pedsdata(day = date.today()):
    """ æŒ‡å®šã—ãŸæ—¥ã«ã¡ã‹ã‚‰ã€ï¼‘é€±é–“åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ›´æ–°  
    Args:
        day(Date) : æ—¥ï¼ˆåˆæœŸå€¤ï¼šä»Šæ—¥ï¼‰
    """ 
    for place_id in range(1, len(name_header.PLACE_LIST) + 1):
        print("[WeeklyUpdate]" + name_header.PLACE_LIST[place_id -1] + " RaceResults")
        update_peds_dataset(place_id, day)
        merge_pedsdata_with_race_results(place_id, day.year)

def monthly_update_pedsdata(day = date.today()):
    """ æŒ‡å®šã—ãŸæ—¥ã«ã¡ã¾ã§ã®ãã®å¹´ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ›´æ–°  
    Args:
        day(Date) : æ—¥ï¼ˆåˆæœŸå€¤ï¼šä»Šæ—¥ï¼‰
    """ 
    for place_id in range(1, len(name_header.PLACE_LIST) + 1):
        print("[MonthlyUpdate]" + name_header.PLACE_LIST[place_id -1] + " PedsResults")
        make_peds_dataset_from_race_results(place_id, day.year)
        merge_pedsdata_with_race_results(place_id, day.year)

def make_all_pedsdata(year = date.today().year):
    """ æŒ‡å®šã—ãŸå¹´ã¾ã§ã®ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ 
    Args:
        day(Date) : æ—¥ï¼ˆåˆæœŸå€¤ï¼šä»Šæ—¥ï¼‰
    """ 
    for y in range(2019, year + 1):
        for place_id in range(1, len(name_header.PLACE_LIST) + 1):
            print("[NewMake]" + str(y) + ":" + name_header.PLACE_LIST[place_id -1] + " PedsResults")
            make_peds_dataset_from_race_results(place_id, y)
            merge_pedsdata_with_race_results(place_id, y)

def make_peds_results(year = date.today().year):
    for place_id in range(1, len(name_header.PLACE_LIST) + 1):
        aggregate_peds_results(place_id, year)

if __name__ == "__main__":
#    make_peds_dataset_from_race_results(3, 2024)
#    make_peds_dataset_from_race_results(5, 2024)
#    make_peds_dataset_from_race_results(8, 2024)
   for place_id in range(1, len(name_header.PLACE_LIST) + 1):
       update_peds_dataset(place_id)
#    make_all_pedsdata()